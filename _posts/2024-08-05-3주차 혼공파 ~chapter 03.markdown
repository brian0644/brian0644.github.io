---
layout: post
title:  "ML 3주차 정리"
date:   2024-08-05 16:53 +09:00
categories: khuda ML session
---
# **3. 회귀 알고지름과 모델 규제**
## 3-1. K-최근접 이웃 회귀
 * 지도학습은 회귀와 분류로 나뉜다. 분류는 어느 집단인지(도미, 빙어)를 분류하는 것, 예측은 임의의 어떤 숫자를 예측하는 문제 -> 연속형 변수와 범주형 변수
 * 결정계수 : 분류의 경우 테스 세트에 있는 샘픙를 정확하게 분류한 개수의 비율, 그러나 회귀에서는 정확하게 숫자를 맞힌다는 것은 불가능 -> 결정계수를 통해 평가, 높을수록 좋음
  
  
### KNN 회귀
  특정 특성을 예측하기 위해 근접하는 k개의 데이터를 살펴보고 예측하는법. 주로 주변 값들의 평균값으로 계산
   
```python
import numpy as np
import matplotlib.pyplot as plt
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
import matplotlib.pyplot as plt
plt.scatter(perch_length, perch_weight)              # 산점도를 통해 길이와 무게의 관계 파악



from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42) # 길이라는 데이터 대한 타깃을 무게로 잡은것
# 현재 훈련세트는 길이로, 1차원 배열에 해당 -> 2차원 배열로 수정할 필요

train_input = train_input.reshape(-1, 1)                       # 1차원 훈련 세트를 2차원으로 확장
test_input = test_input.reshape(-1, 1)                         # 1차원 테스트 세트를 2차원으로 확장


from sklearn.neighbors import KNeighborsRegressor              # 최근접 이웃 회귀 알고리즘 임포트
knr = KNeighborsRegressor()                                    # 모델생성
knr.fit(train_input, train_target)                             # 학습

test_predict = knr.predict(test_input)                         # 테스트 세트에 대한 예측 
mae = mean_absolute_error(test_target, test_predict)           # 테스트 세트에 대한 평균 절댓값 오차 계산 
print(mae)


print(knr.score(test_input, test_target))           
print(knr.score(train_input, train_target))                    # 두 값을 비교하면서 모델이 훈련세트에 대해 과대적합 됐는지 과소적합 됐는지 확인 -> 테스트 점수가 더 높거나 둘다 낮으면 과소, 훈련 점수가 더 높으면 과대적합
```

 **결과 : 테스트 세트의 점수가 더 높기 때문에 과소적합 -> 모델을 더 복잡하게 만들 필요가 있음 -> k의 수를 줄이는 것(훈련 세트에 있는 패턴에 민감해짐 = 약간 과대적합되도록)**
   
```python
knr.n_neighbors = 3                                            # k값 축소 후 결정계수 값 비교
knr.fit(train_input, train_target)
print(knr.score(train_input, train_target))
print(knr.score(test_input, test_target))
```

**정리하자면**<br>
1. 데이터를 2차원 배열로 준비하기 (1차원 두개를 합쳐서 훈련 및 테스트로 쓰는 것이 아닌 1차원 2개에서 1개는 데이터, 1개는 타깃으로 쓰이므로 일차원 배열 두개를 각각 2차원으로 확장 by reshape)
2. kn = KNeighborsRegressor()으로 학습모델 생성 후 kn.fit(데이터, 정답)로 학습
3. kn.score(데이터, 정답)으로 학습 잘 되었는지 확인 -> 훈련 데이터 훈련 타깃에 대한 값, 테스트 데이터 테스트 타깃에 대한 값 비교로 과대적합 과소적합 여부 판단
4. 과소적합이라면 모델을 더 복잡하게 -> k 개수 줄이기 //  과대적합이라면 k를 늘리는 방법도 가능 (별도로 mae = mean_absolute_error(test_target, test_predict)를 통해 오차 절댓값 계산)




------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 

## 3-2 선형 회귀
* knn의 한계 : 해당 산점도를 보면 길이가 커질수록 무게가 증가하는 경향이 있지만 knn은 그저 주변 데이터들의 평균값을 계산할 뿐 -> 오류 발생
* 선형회귀 : 모델의 특성을 가장 잘 표현하는 직선을 토대로 예측하는 알고리즘

<img width="240" alt="스크린샷 2024-08-05 오후 6 04 58" src="https://github.com/user-attachments/assets/c8d33e94-dfdd-4430-b638-931620f8b262">
<img width="222" alt="스크린샷 2024-08-05 오후 6 08 50" src="https://github.com/user-attachments/assets/5574f80e-8fb4-4662-888f-9fee09bd7e5a">

<br>

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()                                        # 선형 회귀 모델 생성
lr.fit(train_input, train_target)                              # 모델 학습  
print(lr.predict([[50]]))                                      # 이전 길이 50에 대한 무게 예측
print(lr.coef_, lr.intercept_)                                 # coef는 기울기, 가중치라고도 불림, 기울기와 y절편값 확인

plt.scatter(train_input, train_target)
plt.plot([15,50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])   # 직선 그리기
plt.scatter(50,1241.8, marker='^')


print(lr.score(train_input, train_target))                     # 훈련세트와 테스트 세트의 결정계수 값 확인으로 과대적합 과소적합 확인 -> 두 값 모두 작아서 과소적합
print(lr.score(test_input, test_target))
```

 **결과 : 훈련 세트와 테스트 세트에 대한 결정 계수 값 둘다 낮음 -> 산점도를 보니 사실 직선보다 최소 2차함수가 필요함을 확인 가능 -> 데이터들도 제곱해줄 필요 있음**

 ```python
train_poly = np.column_stack((train_input **2, train_input))  # 데이터에 제곱을 한 열과 원본 데이터 두개를 2차원으로 붙이기
test_poly = np.column_stack((test_input **2, test_input))

lr_2 = LinearRegression()
lr_2.fit(train_poly, train_target)                            # 단, 타깃은 그대로 사용해야함, 목표하는 값은 어떤 그래프를 훈련하든지 바꿀 필요가 없다. -> 2차식부터는 다항회귀

point = np.arange(15, 50)                                     # 그래프 그리기
plt.scatter(train_input, train_target)
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)
plt.scatter([50],[1574], marker='^')

```
**정리하자면**<br>
1. knn 회귀는 아무리 멀리 떨어져 있더라도 무조건 가장 가까운 샘플의 타깃을 평균하여 예측 -> 그 대안으로 나온 것이 선형회귀와 다항회귀
2. LinearRegression()를 통해 모델 생성, 학습 // 2차 이상으로 그리고 싶다면 column_stack을 통해 데이터 제곱한 것 넣기
3. coef_, intercept_ (모델 파라미터) 를 통해 기울기와 y절편 확인 후 그래프화

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 

## 3-3 특성 공학과 규제
* 다중 회귀 : 특성이 1개라면 직선의 모습, 2개의 특성이라면 평면의 모습 -> 특성이 늘어날 수록 매우 복잡한 모델 표현 가능 -> 기존 길이 뿐만 아니라 높이, 두께, 길이x높이 등 새로운 특성 생성 (특성공학)
* 판다스 : 데이터 분석 라이브러레 -> 데이터프레임을 통해 다차원 배열을 다룰 수 있게 해주며 데이터 프레임은 넘파이 배열로도 쉽게 바꿀 수 있음
* 사이킷런의 변환기 : 특성을 만들거나 전처리를 하기 위한 다양한 클래스를 제공, 이들을 "변환기"라고 부름. fit(),transform() 등의 메서드 제공
* 규제 : 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것. 과대적합 방지 -> 선형 회귀 모델에 규제를 적용할 때 계수 값의 크기가 서로 많이 다르면 공정하게 제어 불가 -> 정규화 우선 후 규제, 이를 릿지, 라쏘라고 부름
* 릿지 : 계수를 제곱한 값을 기준으로 규제 적용 -> 일반적으로 선호
* 라쏘 : 계수의 절댓값을 기준으로 규제 적용 



```python
import pandas as pd
import ssl

ssl._create_default_https_context = ssl._create_unverified_context
df = pd.read_csv("https://bit.ly/perch_csv")

perch_full = df.to_numpy()          # 데이터를 판다스를 통해 데이터프레임으로 불러오고 넘파이배열로 변환


perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])                     # 타깃 데이터 준비

train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42) # 훈련용과 테스트용으로 분리

from sklearn.preprocessing import PolynomialFeatures    # PolynomialFeatures 클래스 임포트

poly = PolynomialFeatures(include_bias=False)           
poly.fit(train_input)                                   # train_input 데이터를 사용하여 다항식 특성 변환을 위한 모델을 학습. 여기서 fit 메서드는 주어진 데이터의 특성의 범위와 다항식 변환에 필요한 정보를 학습
train_poly = poly.transform(train_input)                # train_input 데이터를 다항식 특성으로 변환. 이 메서드는 train_input에 있는 각 특성의 다항식 조합을 생성하여 새로운 특성 행렬을 생성.
print(train_poly.shape)                                 # 데이터는 42개, 특성은 9개인것을 확인가능

name=poly.get_feature_names_out()                       # poly.get_feature_names_out 을 통해 다항 특성 파악
print(name)

test_poly = poly.transform(test_input)                  # test_input 데이터를 다항식 특성으로 변환. 이 메서드는 test_input에 있는 각 특성의 다항식 조합을 생성하여 새로운 특성 행렬을 생성.

lr=LinearRegression()                                   # 모델 생성
lr.fit(train_poly, train_target)                        # 다중 특성 훈련 데이터와 타깃 데이터로 학습
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))                 # 훈련 세트와 테스트 세트 결정 계수 비교 -> 과대적합 과소적합 여부 판단

poly = PolynomialFeatures(degree=5, include_bias=False) # 특성을 많이 만들어서 재수행해보면 너무 많아도 과대적합이 발생한 다는 것을 파악 가능
```

**결과 : 특성이 너무 많아져서 과대적합 발생 -> 규제 적용 필요**
<img width="604" alt="스크린샷 2024-08-05 오후 7 53 38" src="https://github.com/user-attachments/assets/fbba37d7-5c95-4e28-9ec6-4a10bab42d8a">


```python
ss = StandardScaler()                                   # 이와같은 과대적합을 피하고자 규제 전 정규화
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)

*---------------------------------------------------------------------------------------------------------------------------------------*

from sklearn.linear_model import Ridge                  # 릿지 모델 임포트
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))          # 훈련 세트와 테스트 세트 비교
print(ridge.score(test_scaled, test_target))            # 릿지와 라쏘 모델을 사용할 때 규제의 양을 임의로 조절 가능. 모델 객체를 만들 때 alpha 매개변수로 규제의 강도 조절(하이퍼 파라미터).


train_score = []
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]

for alpha in alpha_list:
       ridge = Ridge(alpha=alpha)                # 릿지 모델 생성
       ridge.fit(train_scaled, train_target)     # 릿지 모델 훈련
       train_score.append(ridge.score(train_scaled, train_target)) # 훈련 점수
       test_score.append(ridge.score(test_scaled, test_target))    # 테스트 점수 저장   

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.show()                                       # 그래프를 통해 적정 alpha 값 찾기

ridge = Ridge(alpha=0.1)                         # 해당 alpha로 릿지 모델 훈련 
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

*---------------------------------------------------------------------------------------------------------------------------------------*

lasso = Lasso()                                  # 이번엔 라쏘 모델 생성
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))

train_score = []
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]

for alpha in alpha_list:
       lasso = Lasso(alpha=alpha)                # 라쏘 모델 생성
       lasso.fit(train_scaled, train_target)     # 라쏘 모델 훈련
       train_score.append(lasso.score(train_scaled, train_target)) # 훈련 점수
       test_score.append(lasso.score(test_scaled, test_target))    # 테스트 점수 저장

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.show()                                       # 적정 alpha 값 확인 -> 10

lasso = Lasso(alpha=10)
lasso.fit(train_scaled,train_target)
print(lasso.score(train_scaled,train_target))
print(lasso.score(test_scaled,test_target))

print(np.sum(lasso.coef_ == 0))                  # 라쏘 모델에서 몇개의 계수가 0이 되었는지 확인 가능 -> 주입된 특성 중 실제로 사용된 특성 개수 헤아리기

```

**정리하자면**<br>
1. 일반적인 선형회귀에서는 모델의 복잡성이 낮다는 문제가 있었음 -> 다중 회귀 필요
2. 판다스를 통해 여러 특성을 csv로 받아오고 이를 넘파이 배열로 변환하면 데이터와 타깃 준비 완료
3. poly = PolynomialFeatures(include_bias=False), poly.fit으로 우선 데이터 학습시키고 poly.transform으로 새로운 특성 생성(특성공학)
4. 특성이 많아지면 복잡성은 증가하나 그만큼 훈련 세트에 대한 과대적합 발생  -> 규제 필요 (릿지, 라쏘)
5. 릿지 라쏘 각각의 하이퍼파라미터인 alpha의 적정값을 찾고자 수시로 변경해보면서 훈련세트와 테스트 세트의 값 차이 확인 









