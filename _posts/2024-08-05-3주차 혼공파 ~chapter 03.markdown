---
layout: post
title:  "ML 3주차 정리"
date:   2024-08-05 16:53 +09:00
categories: khuda ML session
---
# **3. 회귀 알고지름과 모델 규제**
## 3-1. K-최근접 이웃 회귀
 * 지도학습은 회귀와 분류로 나뉜다. 분류는 어느 집단인지(도미, 빙어)를 분류하는 것, 예측은 임의의 어떤 숫자를 예측하는 문제 -> 연속형 변수와 범주형 변수
 * 결정계수 : 분류의 경우 테스 세트에 있는 샘픙를 정확하게 분류한 개수의 비율, 그러나 회귀에서는 정확하게 숫자를 맞힌다는 것은 불가능 -> 결정계수를 통해 평가, 높을수록 좋음
  
  
### KNN 회귀
  특정 특성을 예측하기 위해 근접하는 k개의 데이터를 살펴보고 예측하는법. 주로 주변 값들의 평균값으로 계산
 
```python
import numpy as np
import matplotlib.pyplot as plt
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
import matplotlib.pyplot as plt
plt.scatter(perch_length, perch_weight)              # 산점도를 통해 길이와 무게의 관계 파악



from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42) # 길이라는 데이터 대한 타깃을 무게로 잡은것
# 현재 훈련세트는 길이로, 1차원 배열에 해당 -> 2차원 배열로 수정할 필요

train_input = train_input.reshape(-1, 1)                       # 1차원 훈련 세트를 2차원으로 확장
test_input = test_input.reshape(-1, 1)                         # 1차원 테스트 세트를 2차원으로 확장


from sklearn.neighbors import KNeighborsRegressor              # 최근접 이웃 회귀 알고리즘 임포트
knr = KNeighborsRegressor()                                    # 모델생성
knr.fit(train_input, train_target)                             # 학습

test_predict = knr.predict(test_input)                         # 테스트 세트에 대한 예측 
mae = mean_absolute_error(test_target, test_predict)           # 테스트 세트에 대한 평균 절댓값 오차 계산 
print(mae)


print(knr.score(test_input, test_target))           
print(knr.score(train_input, train_target))                    # 두 값을 비교하면서 모델이 훈련세트에 대해 과대적합 됐는지 과소적합 됐는지 확인 -> 테스트 점수가 더 높거나 둘다 낮으면 과소, 훈련 점수가 더 높으면 과대적합
```

 **결과 : 테스트 세트의 점수가 더 높기 때문에 과소적합 -> 모델을 더 복잡하게 만들 필요가 있음 -> k의 수를 줄이는 것(훈련 세트에 있는 패턴에 민감해짐 = 약간 과대적합되도록)**
   
```python
knr.n_neighbors = 3                                            # k값 축소 후 결정계수 값 비교
knr.fit(train_input, train_target)
print(knr.score(train_input, train_target))
print(knr.score(test_input, test_target))
```

**정리하자면**<br>
1. 데이터를 2차원 배열로 준비하기 (1차원 두개를 합쳐서 훈련 및 테스트로 쓰는 것이 아닌 1차원 2개에서 1개는 데이터, 1개는 타깃으로 쓰이므로 일차원 배열 두개를 각각 2차원으로 확장 by reshape)
2. kn = KNeighborsRegressor()으로 학습모델 생성 후 kn.fit(데이터, 정답)로 학습
3. kn.score(데이터, 정답)으로 학습 잘 되었는지 확인 -> 훈련 데이터 훈련 타깃에 대한 값, 테스트 데이터 테스트 타깃에 대한 값 비교로 과대적합 과소적합 여부 판단
4. 과소적합이라면 모델을 더 복잡하게 -> k 개수 줄이기 //  과대적합이라면 k를 늘리는 방법도 가능 (별도로 mae = mean_absolute_error(test_target, test_predict)를 통해 오차 절댓값 계산)




------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 

## 3-2 선형 회귀
* knn의 한계 : 해당 산점도를 보면 길이가 커질수록 무게가 증가하는 경향이 있지만 knn은 그저 주변 데이터들의 평균값을 계산할 뿐 -> 오류 발생
* 선형회귀 : 모델의 특성을 가장 잘 표현하는 직선을 토대로 예측하는 알고리즘

<img width="240" alt="스크린샷 2024-08-05 오후 6 04 58" src="https://github.com/user-attachments/assets/c8d33e94-dfdd-4430-b638-931620f8b262">
<img width="222" alt="스크린샷 2024-08-05 오후 6 08 50" src="https://github.com/user-attachments/assets/5574f80e-8fb4-4662-888f-9fee09bd7e5a">

<br>

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()                                        # 선형 회귀 모델 생성
lr.fit(train_input, train_target)                              # 모델 학습  
print(lr.predict([[50]]))                                      # 이전 길이 50에 대한 무게 예측
print(lr.coef_, lr.intercept_)                                 # coef는 기울기, 가중치라고도 불림, 기울기와 y절편값 확인

plt.scatter(train_input, train_target)
plt.plot([15,50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])   # 직선 그리기
plt.scatter(50,1241.8, marker='^')


print(lr.score(train_input, train_target))                     # 훈련세트와 테스트 세트의 결정계수 값 확인으로 과대적합 과소적합 확인 -> 두 값 모두 작아서 과소적합
print(lr.score(test_input, test_target))
```

 **결과 : 훈련 세트와 테스트 세트에 대한 결정 계수 값 둘다 낮음 -> 산점도를 보니 사실 직선보다 최소 2차함수가 필요함을 확인 가능 -> 데이터들도 제곱해줄 필요 있음**

 ```python
train_poly = np.column_stack((train_input **2, train_input))  # 데이터에 제곱을 한 열과 원본 데이터 두개를 2차원으로 붙이기
test_poly = np.column_stack((test_input **2, test_input))

lr_2 = LinearRegression()
lr_2.fit(train_poly, train_target)                            # 단, 타깃은 그대로 사용해야함, 목표하는 값은 어떤 그래프를 훈련하든지 바꿀 필요가 없다. -> 2차식부터는 다항회귀

point = np.arange(15, 50)                                     # 그래프 그리기
plt.scatter(train_input, train_target)
plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)
plt.scatter([50],[1574], marker='^')

```
**정리하자면**<br>
1. knn 회귀는 아무리 멀리 떨어져 있더라도 무조건 가장 가까운 샘플의 타깃을 평균하여 예측 -> 그 대안으로 나온 것이 선형회귀와 다항회귀
2. LinearRegression()를 통해 모델 생성, 학습 // 2차 이상으로 그리고 싶다면 column_stack을 통해 데이터 제곱한 것 넣기
3. coef_, intercept_ (모델 파라미터) 를 통해 기울기와 y절편 확인 후 그래프화

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 

## 3-3 특성 공학과 규제
* 다중 회귀 : 특성이 1개라면 직선의 모습, 2개의 특성이라면 평면의 모습 -> 특성이 늘어날 수록 매우 복잡한 모델 표현 가능 -> 기존 길이 뿐만 아니라 높이, 두께, 길이x높이 등 새로운 특성 생성 (특성공학)
* 판다스 : 데이터 분석 라이브러레 -> 데이터프레임을 통해 다차원 배열을 다룰 수 있게 해주며 데이터 프레임은 넘파이 배열로도 쉽게 바꿀 수 있음
* 사이킷런의 변환기 : 특성을 만들거나 전처리를 하기 위한 다양한 클래스를 제공, 이들을 "변환기"라고 부름. fit(),transform() 등의 메서드 제공

```python
import pandas as pd
import ssl

ssl._create_default_https_context = ssl._create_unverified_context
df = pd.read_csv("https://bit.ly/perch_csv")

perch_full = df.to_numpy()          # 데이터를 판다스를 통해 데이터프레임으로 불러오고 넘파이배열로 변환


perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])                     # 타깃 데이터 준비

train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42) # 훈련용과 테스트용으로 분리

from sklearn.preprocessing import PolynomialFeatures    # PolynomialFeatures 클래스 임포트

poly = PolynomialFeatures(include_bias=False)           
poly.fit(train_input)                                   # train_input 데이터를 사용하여 다항식 특성 변환을 위한 모델을 학습. 여기서 fit 메서드는 주어진 데이터의 특성의 범위와 다항식 변환에 필요한 정보를 학습
train_poly = poly.transform(train_input)                # train_input 데이터를 다항식 특성으로 변환. 이 메서드는 train_input에 있는 각 특성의 다항식 조합을 생성하여 새로운 특성 행렬을 생성.
print(train_poly.shape)                                 # 데이터는 42개, 특성은 9개인것을 확인가능

name=poly.get_feature_names_out()
print(name)

```

**정리하자면**<br>
1. 1차원 리스트를 column_stack으로 나란히 인덱스에 맞춰서 붙이기 -> 넘파이 2차원 배열로 생성
2. 훈련세트와 테스트 세트를 train_test_split함수와 stratify=를 통해 원래 타깃 정답 비율과 맞춰서 나눠주기
3. 평균 표준편차를 구하고 정규화 해주기 -> 테스트 세트는 훈련세트 평균과 표준편차에 맞춰서 스케일 조정해주기
4. 모델 학습시키기 (fit)










