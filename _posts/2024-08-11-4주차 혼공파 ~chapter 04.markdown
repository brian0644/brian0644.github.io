---
layout: post
title:  "ML 4주차 정리"
date:   2024-08-11 14:38 +09:00
categories: khuda ML session
---

# 4. 다양한 분류 알고리즘
## 4-1. 로지스틱 회귀

* 로지스틱 회귀 : 이름은 회귀이지만 분류 모델, 선형 회귀와 동일하게 선형 방정식을 학습
* 로지스틱 다중분류 : LogisticRegression 클래스는 기본적으로 반복적인 알고리즘 사용. max_iter 매개변수에서 반복 횟수를 지정하며 기본값은 100.
+ 또한 기본적으로 릿지 회귀(L2)와 같이 계수의 제곱을 규제 -> 매개변수 C로 조절하며 작을수록 규제가 커짐. 기본값은 1
+ 다중분류는 시그모이드가 아닌 소프트맥스(시그모이드는 하나의 선형 방정식의 출력값을 0~1 사이로 압축하지만 소프트맥스는 여러개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만듬) 함수를 사용 -> 정규화된 지수함수라고 부름

<img width="653" alt="스크린샷 2024-08-11 오후 3 38 41" src="https://github.com/user-attachments/assets/5fdb9db6-b126-4871-a1fa-ff5f74c5a04f">  

-> a,b,c,d,e는 가중치 혹은 계수. 특성은 전의 ax+b보다 늘어났지만 다중 회귀를 위한 선형방정식과 같다.
z가 확률 값이 되려면 0~1 사이의 값이 되어야 한다. z가 아주 큰 음수일 때 0, 아주 큰 양수일 때 1이 되도록 바꾸는 것이 시그모이드 함수(로지스틱 함수)  

<img width="552" alt="스크린샷 2024-08-11 오후 3 40 30" src="https://github.com/user-attachments/assets/7f2214af-ef8d-42a4-93ff-625d71564a4c">

-> 선형 방정식의 출력 z를 시그모이드 함수의 입력으로써 사용해 z가 큰 음수일 경우 0에 수렴해지고 큰 양수일 경우 1에 가깝도록 만든 것

```python

# 우선 시그모이드 함수를 그려보자
import matplotlib.pyplot as plt
z = np.arange(-5,5,0.1)     # 함수 범위 세팅
phi = 1 / (1+np.exp(-z))    # 범위 세팅한 z를 다시 시그모이드 함수의 정의역으로 입력
plt.plot(z, phi)            # 함수 그리기
```

-> 시그모이드 함수는 이진 분류에서 출력값이 0.5 보다 클때 양성 클래스, 0.5 보다 작으면 음성 클래스로 판단. 우선 이진 분류부터 확인


```python
# 도미와 빙어의 행만 골라내서 데이터 생성
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]


# 해당 데이터로 로지스틱 회귀 모델 훈련
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
lr.predict(train_bream_smelt[:5])

# 처음 5개 샘플의 예측 확률을 출력
lr.predict_proba(train_bream_smelt[:5])

# z를 만들기 위한 가중치와 편향 구하기
print(lr.coef_,lr.intercept_)
```
<img width="635" alt="스크린샷 2024-08-11 오후 4 32 10" src="https://github.com/user-attachments/assets/5540037b-4c86-4111-ad80-b3bacb92dc87">




```python
# C를 통해 규제 정도 조절, max_iter을 통해 반복회수 조절
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

# 테스트 세트의 5개 데이터에 대한 예측값, 예측 과정 살펴보기
proba = lr.predict_proba(test_scaled[:5])
np.round(proba, decimals=3)

# 다중 분류에 대한 가중치와 편향 값 살펴보기
print(lr.coef_.shape, lr.intercept_.shape)
# -> 가중치는 7개 타깃에 대하여 5개의 특성(길이, 무기..)를 조사하였으며 편향도 마찬가지
# 이것은 각 7개의 타깃에 대하여 계산을 7번 조사한것
# 다중 분류는 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환
```

  
<img width="453" alt="스크린샷 2024-08-11 오후 4 57 31" src="https://github.com/user-attachments/assets/4bcb343d-307e-4cfd-9580-2db9968f8b8a">
<img width="428" alt="스크린샷 2024-08-11 오후 4 57 34" src="https://github.com/user-attachments/assets/1c940828-14d2-4cdb-85c7-c950b0d2210b">


```python
# 소프트맥스 함수의 출력값이 어떻게 계산됐는지 z1~z7까지의 값을 구한 다음 소프트맥스 함수를 통해 확률로 바꿔보기
decisions = lr.decision_function(test_scaled[:5])
np.round(decisions, decimals=2)
# 샘플 5개에 대한 타깃 7개 각각의 z값을 계산


# 사이파이로도 결과 계산 가능
from scipy.special import softmax
proba = softmax(decisions, axis=1)
np.round(proba, decimals=3)
```

## 4-2. 확률적 경사 하강법
* 확률전 경사 하강법 : 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표이며 무작위 하나의 샘플에 대한 경사를 하나씩 확인하면서 천천히 내려오는 방법 -> 확률적 경사 하강법을 거친 이후에도 극소값에 도달하지 못했다면 다시 랜덤하게 경를 내려간다.
* 에포크 : 확률적 경사 하강법에서 훈련세트를 한 번 모두 사용하는 과정 -> 일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행
* 미니배치 경사 하강법 :  무작위 하나의 샘플이 아닌 무작위 하나의 배치(여러개 샘플의 묶음)을 사용해 수행하는 방식
* 배치 경사 하강법 : 극단적으로 한번에 전체 샘플을 사용하는 방법
<img width="800" alt="스크린샷 2024-08-11 오후 5 34 46" src="https://github.com/user-attachments/assets/cd8043ec-1d0e-415c-b64d-04b9394dc167">

**신경망 알고리즘에서는 확률적 경사 하강법이 필수적으로 사용됨 <- 신경망은 일반적으로 많은 데이터를 사용하기 때문에 한번에 모든 데이터를 사용하기 어려우며 모델이 매우 복잡하다는 점에서 수학적인 방법으로 해답을 얻기 어려움**


* 손실함수 : 머신러닝 알고리즘을 평가하는 척도. 작을수록 좋음 -> 손실 함수의 값이 작아지도록 경사하강법을 사용해야하며 이때 손실함수는 "미분가능" 해야한다 -> 이산적인 값을 연속으로 바꿔줘야함
* 로지스틱 손실 함수 : 이진 분류에서 사용되는 손실 함수, 이진 크로스엔트로피 손실 함수라고도 불림
* 크로스엔트로피 손실 함수 : 다중 분류에서 사용되는 손실 함수
* 이밖에도 회귀의 손실함수로는 평균 절댓값 오차 혹은 평균 제곱 오차를 많이 사용한다







