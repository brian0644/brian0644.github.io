---
layout: post
title:  "ML 4주차 정리"
date:   2024-08-11 14:38 +09:00
categories: khuda ML session
---

# 4. 다양한 분류 알고리즘
## 4-1. 로지스틱 회귀

* 로지스틱 회귀 : 이름은 회귀이지만 분류 모델, 선형 회귀와 동일하게 선형 방정식을 학습
* 로지스틱 다중분류 : LogisticRegression 클래스는 기본적으로 반복적인 알고리즘 사용. max_iter 매개변수에서 반복 횟수를 지정하며 기본값은 100.
+ 또한 기본적으로 릿지 회귀(L2)와 같이 계수의 제곱을 규제 -> 매개변수 C로 조절하며 작을수록 규제가 커짐. 기본값은 1
+ 다중분류는 시그모이드가 아닌 소프트맥스(시그모이드는 하나의 선형 방정식의 출력값을 0~1 사이로 압축하지만 소프트맥스는 여러개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만듬) 함수를 사용 -> 정규화된 지수함수라고 부름

<img width="653" alt="스크린샷 2024-08-11 오후 3 38 41" src="https://github.com/user-attachments/assets/5fdb9db6-b126-4871-a1fa-ff5f74c5a04f">  

-> a,b,c,d,e는 가중치 혹은 계수. 특성은 전의 ax+b보다 늘어났지만 다중 회귀를 위한 선형방정식과 같다.
z가 확률 값이 되려면 0~1 사이의 값이 되어야 한다. z가 아주 큰 음수일 때 0, 아주 큰 양수일 때 1이 되도록 바꾸는 것이 시그모이드 함수(로지스틱 함수)  

<img width="552" alt="스크린샷 2024-08-11 오후 3 40 30" src="https://github.com/user-attachments/assets/7f2214af-ef8d-42a4-93ff-625d71564a4c">

-> 선형 방정식의 출력 z를 시그모이드 함수의 입력으로써 사용해 z가 큰 음수일 경우 0에 수렴해지고 큰 양수일 경우 1에 가깝도록 만든 것

```python

# 우선 시그모이드 함수를 그려보자
import matplotlib.pyplot as plt
z = np.arange(-5,5,0.1)     # 함수 범위 세팅
phi = 1 / (1+np.exp(-z))    # 범위 세팅한 z를 다시 시그모이드 함수의 정의역으로 입력
plt.plot(z, phi)            # 함수 그리기
```

-> 시그모이드 함수는 이진 분류에서 출력값이 0.5 보다 클때 양성 클래스, 0.5 보다 작으면 음성 클래스로 판단. 우선 이진 분류부터 확인


```python
# 도미와 빙어의 행만 골라내서 데이터 생성
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]


# 해당 데이터로 로지스틱 회귀 모델 훈련
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
lr.predict(train_bream_smelt[:5])

# 처음 5개 샘플의 예측 확률을 출력
lr.predict_proba(train_bream_smelt[:5])

# z를 만들기 위한 가중치와 편향 구하기
print(lr.coef_,lr.intercept_)
```
<img width="635" alt="스크린샷 2024-08-11 오후 4 32 10" src="https://github.com/user-attachments/assets/5540037b-4c86-4111-ad80-b3bacb92dc87">




```python
# C를 통해 규제 정도 조절, max_iter을 통해 반복회수 조절
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

# 테스트 세트의 5개 데이터에 대한 예측값, 예측 과정 살펴보기
proba = lr.predict_proba(test_scaled[:5])
np.round(proba, decimals=3)

# 다중 분류에 대한 가중치와 편향 값 살펴보기
print(lr.coef_.shape, lr.intercept_.shape)
# -> 가중치는 7개 타깃에 대하여 5개의 특성(길이, 무기..)를 조사하였으며 편향도 마찬가지
# 이것은 각 7개의 타깃에 대하여 계산을 7번 조사한것
# 다중 분류는 소프트맥스 함ㅅ를 사용하여 7개의 z값을 확률로 변환


```
