---
layout: post
title:  "ML 5주차 정리"
date:   2024-08-19 15:54 +09:00
categories: khuda ML session
---

# 5. 트리 알고리즘
## 5-1. 결정 트리
* 결정트리 :
  다른 분류 모델보다 이해하기 쉬우며 루트 노드와 리프 노드로 이루어져있다
  루트노드에서 쓰인 특성이 가장 유용한 특성 중 하나로 쓰이는 척도가 될 수 있다

    
<img width="723" alt="스크린샷 2024-08-19 오후 4 15 51" src="https://github.com/user-attachments/assets/169366b5-fb7b-4db1-b5c4-209c9f000baa">

    
* 불순도 :
  **gini**
  gini는 지니 불순도를 의미한다. 매개변수 기본값이 gini이며 지니 불순도는 1-(음성 클래스 비율^2 + 양성 클래스 비율^2) 로 이루어져있다.
  결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록(한쪽 클래스의 비율을 더 높이도록) 트리를 성장시킨다
  먼저 자식 노드의 불순도를 샘플 개수에 비례하여 모두 더하고 부모 노드의 불순도에서 뺀다
  이런 부모와 자식 노드 사이의 불순도 차리를 **정보 이득** 이라고 부른다.

  **entropy**
  entropy는 엔트로피 불순도를 의미한다. 엔트로피 불순도도 노드의 클래스 비율을 사용하지만 지니 불순도처럼 제곱이 아니라 밑이 인 로그를 사용하여 곱한다.


* 가지치기 :
  결정트리의 규제는 가지치기를 통해 이루어진다. 훈련세트에 적합한 노드를 일부 제거함으로써 과대적합을 방지하는 원리
  가지치기를 하는 가장 간단한 방법은 자라날 수 있는 트리의 최대 깊이를 지정하는 것 -> **DecisionTreeClassfier 클래스의 max_depth 매개변수를 조정**


```python
# 결정트리 패키지 임포트 후 학습
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
# -> train점수가 test보다 높기 때문에 과대적합, 규제할 필요 있음


# plot tree를 통해 결정 트리 시각화하기
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()


# 그림이 너무 복잡하니 트리의 깊이를 제한해서 출력
# max_depth 변수를 1로 주면 루트 노드를 제외하고 하나의 노드를 더 확장하여 그림
# filled 변수에서 클래스에 맞게 노드 색 칠하기
# feature_names 변수에서 특성의 이름을 전달하기\
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled = True, feature_names=['alcohol','sugar','pH'])
plt.show()
# -> value = [음성 클래스, 양성 클래스], value 값은 분류 결과가 아니라 해당 노드에서 가지는 양음성 클래스 데이터의 개수일뿐


# max_depth = 3으로 지정해서 재학습
dt = DecisionTreeClassifier(max_depth= 3,random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))


# plot tree를 통해 결정 트리 시각화하기
plt.figure(figsize=(10,7))
plot_tree(dt, filled = True, feature_names=['alcohol','sugar','pH'])
plt.show() 


# feature_importances_ 속성에 저장된 특성별 중요도 출력
print(dt.feature_importances_)


# 가지치기 max_depth 말고 min_impurity_decrease 사용해보기
# 어떤 노드의 정보이득 x (노드의 샘플 수) / (전체 샘플 수) 값이 이 매개변수보다 작으면 더이상 분할하지 않음
dt = DecisionTreeClassifier(min_impurity_decrease = 0.0005,random_state=42)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
plt.figure(figsize=(20,15), dpi =300)
plot_tree(dt, filled = True, feature_names=['alcohol','sugar','pH'])
plt.show() 

```
