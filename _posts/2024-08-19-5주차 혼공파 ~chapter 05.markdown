---
layout: post
title:  "ML 5주차 정리"
date:   2024-08-19 15:54 +09:00
categories: khuda ML session
---

# 5. 트리 알고리즘
## 5-1. 결정 트리
* 결정트리 :
  다른 분류 모델보다 이해하기 쉬우며 루트 노드와 리프 노드로 이루어져있다
  루트노드에서 쓰인 특성이 가장 유용한 특성 중 하나로 쓰이는 척도가 될 수 있다
  
    
<img width="723" alt="스크린샷 2024-08-19 오후 4 15 51" src="https://github.com/user-attachments/assets/169366b5-fb7b-4db1-b5c4-209c9f000baa">

    
* 불순도 :

  **gini**
  
  gini는 지니 불순도를 의미한다. 매개변수 기본값이 gini이며 지니 불순도는 1-(음성 클래스 비율^2 + 양성 클래스 비율^2) 로 이루어져있다.
  결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록(한쪽 클래스의 비율을 더 높이도록) 트리를 성장시킨다
  먼저 자식 노드의 불순도를 샘플 개수에 비례하여 모두 더하고 부모 노드의 불순도에서 뺀다
  이런 부모와 자식 노드 사이의 불순도 차리를 **정보 이득** 이라고 부른다.

  **entropy**
  
  entropy는 엔트로피 불순도를 의미한다. 엔트로피 불순도도 노드의 클래스 비율을 사용하지만 지니 불순도처럼 제곱이 아니라 밑이 인 로그를 사용하여 곱한다.
  
  
* 가지치기 :
  결정트리의 규제는 가지치기를 통해 이루어진다. 훈련세트에 적합한 노드를 일부 제거함으로써 과대적합을 방지하는 원리
  가지치기를 하는 가장 간단한 방법은 자라날 수 있는 트리의 최대 깊이를 지정하는 것 -> **DecisionTreeClassfier 클래스의 max_depth 매개변수를 조정**
  
  
```python
# 결정트리 패키지 임포트 후 학습
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
# -> train점수가 test보다 높기 때문에 과대적합, 규제할 필요 있음


# plot tree를 통해 결정 트리 시각화하기
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()


# 그림이 너무 복잡하니 트리의 깊이를 제한해서 출력
# max_depth 변수를 1로 주면 루트 노드를 제외하고 하나의 노드를 더 확장하여 그림
# filled 변수에서 클래스에 맞게 노드 색 칠하기
# feature_names 변수에서 특성의 이름을 전달하기\
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled = True, feature_names=['alcohol','sugar','pH'])
plt.show()
# -> value = [음성 클래스, 양성 클래스], value 값은 분류 결과가 아니라 해당 노드에서 가지는 양음성 클래스 데이터의 개수일뿐


# max_depth = 3으로 지정해서 재학습
dt = DecisionTreeClassifier(max_depth= 3,random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))


# plot tree를 통해 결정 트리 시각화하기
plt.figure(figsize=(10,7))
plot_tree(dt, filled = True, feature_names=['alcohol','sugar','pH'])
plt.show() 


# feature_importances_ 속성에 저장된 특성별 중요도 출력
print(dt.feature_importances_)


# 가지치기 max_depth 말고 min_impurity_decrease 사용해보기
# 어떤 노드의 정보이득 x (노드의 샘플 수) / (전체 샘플 수) 값이 이 매개변수보다 작으면 더이상 분할하지 않음
dt = DecisionTreeClassifier(min_impurity_decrease = 0.0005,random_state=42)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
plt.figure(figsize=(20,15), dpi =300)
plot_tree(dt, filled = True, feature_names=['alcohol','sugar','pH'])
plt.show() 

```



## 5-1. 결정 트리
* 검증 세트 :
  
일반적으로 테스트 세트로 일반화 성능을 올바르게 예측하려면 가능한 한 테스트 세트를 사용하지 말아야한다<br>
모델을 만들고 나서 마지막에 딱 한번만 사용하는 것이 좋다<br>
그렇다면 max_depth 매개변수를 사용한 하이퍼파라미터 튜닝을 어떻게 할 수 있을까 + 결정트리는 테스트해 볼 매개변수가 많다<br>
-> 테스트 세트를 사용하지 않으면 모델이 과대적합인지 과소적합인지 판단하기 어려움 -> 훈련세트를 또 나누어 검증세트로 활용
<br>

* 교차 검증 :
  
보통 많은 데이터를 훈련에 사용할수록 좋은 모델이 만들어짐<br>
그렇다고 검증 세트를 너무 조금 떼어 놓으면 검증 점수가 들쭉날쭉하고 불안정할것<br>
이럴 때 **교차검증**을 이용하면 안정적인 검증 점수를 얻고 훈련에 더 많은 데이터 사용 가능<br>
교차 검증은 검증 세트를 떼어 내어 평가하는 과정을 여러 번 반복함<br>
그다음 이점수를 평균하여 최종 검증 점수를 얻어냄

<img width="539" alt="스크린샷 2024-08-19 오후 4 49 04" src="https://github.com/user-attachments/assets/c4872202-3717-40f9-a462-d9599001245e">

이러한 과정을 3-fold cross check라고 부름 (훈련 세트를 세 부분으로 나누어 교차 검증 수행, k-fold cross check라고 부름)<br>
사이킷 런에는 cross_validate()라는 교차 검증 함수가 존재, 평가할 모델 객체를 첫 번째 매개변수로, 그다음 앞에서처럼 직접 검증 세트를 떼어 내지 않고 훈련 세트 전체를 함수에 전달<br>
cross_validate는 훈련 세트를 섞어서 폴드로 나누지 않기 때문에 분할기(splitter)를 지정해야함


* 하이퍼파라미터 튜닝 :
  
머신러닝 모델에서 사용자가 지정해야만 하는 파라미터를 하이퍼파라미터라고 부름<br>
이런 하이퍼파라미터 튜닝은 모델마다 적게는 1~2개, 많게는 5~6개 제공<br>
이 매개변수를 바꿔가면서 모델을 훈련하고 교차 검증 수행(사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 하는 기술을 'AutoML'이라고 부름)<br>
각 하이퍼파라미터의 최적값은 다른 하이퍼파라미터의 변화에 영향받음 -> 여러개의 매개변수를 동시에 바꿔가며 최적의 값을 찾아야함<br>
for 반복문으로 이런 과정을 직접 구현할 수도 있지만 사이킷런에서 제공하는 **그리드 서치**를 사용하면 됨<br>
사이킷런의 GridSearchCV 클래스는 하이퍼파라미터 탐색과 교차 검증을 한번에 수행 -> cross_validate() 함수를 호출할 필요 없음




```python
# 훈련세트에서 또 20%를 떼와서 검증 세트로 준비
sub_input, val_input, sub_target, val_target = train_test_split(train_input,train_target,test_size=0.2,random_state=42)
print(sub_input.shape, val_input.shape)


# 결정 트리 학습
# 결정트리 패키지 임포트 후 학습
# 이제 훈련된 모델은 검증 세트로 평가, 최종 점수는 테스트 세트로
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))


# k-fold cross check 수행
# cross_validate 함수 임포트 후 (모델, 훈련세트 전체)를 전달
# 각 결과는 fit_time, score_time,test_score 키를 가진 딕셔너리 반환, cv 매개 변수는 k-fold의 k개 결정
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
 

# 검증 세트에 대한 점수 평균치 계산
print(np.mean(scores['test_score']))


# 사이킷런의 분할기는 교차 검증에서 폴드를 어떻게 나눌지 결정
# cross_validate() 함수는 기본적으로 회귀 모델일 경우 KFold 분할기를 사용하고
# 분류 모델일 경우 타깃 클래스를 골고루 나누기 위해 StratifiedKFold를 사용
from sklearn.model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold())
print(np.mean(scores['test_score']))


# 훈련 세트를 섞은 후 10-Fold cross check 수행
# n_splits 는 k를 지정 -> k개로 훈련세트를 나눠야하니까
# shuffle 은 데이터 랜덤하기 섞기
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))



```
