---
layout: post
title:  "금융트랙 1주차 정리"
date:   2024-09-17 15:31+09:00
categories: khuda finance track
---

# 인공신경망 정리
* 인공신경망 구조 : 뉴런, 계층, 가중치
* 인공신경망 훈련 : 순전파, 역전파, 경사 하강법
* 인공신경망 하이퍼파라미터 : 계층, 노드 수, 활성화 함수, 손실 함수, 학습률

-> 입력층, 은닉층, 출력층을 거쳐 Y'가 생성되면 실제 값 Y와의 차이를 손실함수J(w)로 정의, 손실함수의 최적화를 경사 하강법(혹은 모멘텀, RMSProp, 아담)을 통해 손실이 최소가 되는 w 탐색 후 재훈련 과정 반복
** 보통 은닉층 노드 수는 입력층과 출력층 크기의 중간값 정도가 적당, 과적합을 피하기 위해서는 입력층의 두배를 넘지 않는 것이 좋음

# 지도학습 정리
<img width="362" alt="스크린샷 2024-09-17 오후 3 50 10" src="https://github.com/user-attachments/assets/c8f3a0c8-1017-4ce7-9886-0a2a5b81a3aa">

## 선형회귀
입력 변수 x와 단일 출력 변수 y간에 선형 관계가 있다고 가정, RSS가 최소가 되는 값을 기반으로(OLS)

<img width="356" alt="스크린샷 2024-09-17 오후 3 55 52" src="https://github.com/user-attachments/assets/0ebb2efc-2ef0-42ff-ac76-b63b1bc2f505">


* 장점 : 이해하기 쉬움
* 단점 : 비선형인 경우 올바르게 작동하지 않을 수 있ㅇ으며 과적합에 취약, 다중공선성 문제에도 취약


## 로지스틱회귀
출력 클래스의 확률 분포를 모델링 할 수 있다. 사용되는 함수는 x에 대해 선형함수이며 출력 확률은 0과 1사이의 값을 가지고 그 합이 1

* 하이퍼파라미터 : 정규화 L1 or L2 or elasticnet , 정규화 강도 c
* 장점 : 구현하기도 쉽고 해석성 좋다. 선형적으로 분리된 클래스에서 잘 작동한다. 모델의 출력이 확률로 나타나다보니 순위를 정하는데 유용
* 단점 : 특성의 수가 많아지면 모델이 과적합 가능성 있으며 선형 함수만 학습할 수 있고 특성과 목표 변수의 관계가 복잡하면 어려움


## SVM
<img width="353" alt="스크린샷 2024-09-17 오후 4 02 27" src="https://github.com/user-attachments/assets/7f2c109e-8a92-44af-8467-8f33f0690d2a">

마진(결정선과 가장 가까이 있는 훈렴샘플 사이의 거리)을 최대화 하는 것
* 하이퍼파라미터 : 커널, 패널티
* 장점 : 과적합에 안정적이라 고차원 영역일수록 좋음. 비선형 관계를 다루는 커널이 다양하고 데이터의 분포도를 요하지 않ㅇ므
* 단점 : 학습에 비효율적이고 메모리를 많이 요구. 큰 데이터셋에는 성능이 좋지 않으며 데이터의 스케일링이 필요


## KNN
전체 훈련셋을 통해 그 데이터에 가장 근접한 k개의 이웃을 찾고 그 k개에 대한 출력변수를 도출하는 방식. 가장 보편적인 거리 측정법은 n2.

* 하이퍼파라미터 : 이웃 수 k, 거리 측정법 n(1~...)
* 장점 : 학습이 필요없고 학습 단계가 없다는 장점. 새로운 데이터 추가 가능하며 직관적으로 쉽게 이해 가능, 큰 데이터에 효과적, 이상치를 걸러낼 필요 x
* 단점 : 거리 측정법이 불명확하고 많은 경우에 선택하기 어려움. 고차원 데이터셋이서 성능 저하.


## 선형 판별 분석(LDA)
분류 분별력을 극대화하고 분류 내의 분산을 최소화하는 방식으로 데이터를 저차원 영역에 투영한다(PCA와 유사)
LDA 모델을 훈련하는 동안 각 분류에 대한 통계적 특성을 계산하며 이는 데이터가 정규분포를 따르고 각 속성의 분산이 같다는 가정을 기반으로 평가한다

* 장점 : 빠르고 쉽게 구현
* 단점 : 특성 스케일링이 필요하고 복잡한 행렬 연산 요구


## 트리 알고리즘
<img width="357" alt="스크린샷 2024-09-17 오후 4 22 39" src="https://github.com/user-attachments/assets/547c8b66-2576-40f8-a809-86490408e007">

CART모델이라고도 불리며(분류 혹은 예측 트리의 약자) 기준정지, 가지치기 등의 과정으로 훈련을 조절한다

* 하이퍼파라미터 : 정말 다양함. 핵심은 max_depth로, 최대 깊이 조정
* 장점 : 해석하기 쉽고 데이터 스케일링 요구 x, 회귀 및 분류 모두 가능하며 대용량 데이터셋이서도 좋다
* 단점 : 가지치기를 하지 않으면 쉽게 과적합되며 앙상ㄹ 모델보다 거의 더 나쁜 성능을 보인다


## 앙상블 모델
여러 분류기를 합쳐 개개의 분류기보다 성능이 더 좋은 메타 분류기를 만든다. 배깅과 부스팅을 기본적으로 하며 랜덤 포레스트가 가장 핵심적

* 하이퍼파라미터 : 최대 특성 수, 평가자 수 (주요 일부일 뿐)
* 장점 : 우수한 성능, 확장성, 사용성. 특성의 중요도를 점수로 나타낼 수 있으며 중복되는 특성 열을 처리 가능, 데이터 스케일링 필요 없으며 비선형 관계 모델링 가능
* 단점 : 결과를 해석하기 어려움(블랙박스 접근법과 유사)

# 최종 정리
<img width="373" alt="스크린샷 2024-09-17 오후 4 34 11" src="https://github.com/user-attachments/assets/ffc019d0-d044-4be5-83df-d3b0405766f7">
