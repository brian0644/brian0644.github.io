---
layout: post
title:  "금융트랙 3주차 정리"
date:   2024-09-30 15:01+09:00
categories: khuda finance track
---
# 비지도학습: 차원축소
* 비지도 학습 알고리즘은 데이터가 산출하려는 출력에 대한 지식 없이 데이터에서 패턴을 추론한다
* 레이블 데이터를 요하지 않기 때문에 분석과 모델 개발에 더 큰 데이터셋을 쉽게 사용할 수 있다
* 레이블 데이터는 시간이 오래걸리고 생성하거나 획득하는 데 비실용적일 수 있다
* 그 중에서도 차원축는 정보 손실을 최소화하면서 원래 특성에서 가장 중요한 것을 포착하는 변수의 더 작은 셋을 찾는 방법으로 데이터를 압축한다. 차원축소는 높은 차원과 관련된 문제를 완화하는 데 도움이 되며, 탐색하기 어려운 고차원 데이터의 주요 특성을 시각화 할 수 있다
<br>
-> 데이터셋의 잡음과 중복을 줄이고 더 적은 특성을 사용해 데이터셋을 찾아서 고려할 변수를 줄이고 데이터셋의 탐색과 시각화를 간단하게 한다.
<br>
-> 또한 차원 축소 기술은 특성 수를 줄이거나 새로운 특성을 찾아서 지도 학습 기반 모델을 향상시킨다.


## 예제 : 수익률 곡선 구축 및 이자율 모델링

```python
# Load libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

#Import Model Packages
from sklearn.decomposition import PCA

import requests
import pandas as pd

# Replace 'YOUR_FRED_API_KEY' with your actual FRED API key
API_KEY = '72753884a1156f9d72b21142b30e9b70'

# Treasury datasets from FRED
treasury_series = {
    '1 Month': 'DGS1MO',
    '3 Month': 'DGS3MO',
    '6 Month': 'DGS6MO',
    '1 Year': 'DGS1',
    '2 Year': 'DGS2',
    '3 Year': 'DGS3',
    '5 Year': 'DGS5',
    '7 Year': 'DGS7',
    '10 Year': 'DGS10',
    '20 Year': 'DGS20',
    '30 Year': 'DGS30'
}

# Function to get data from FRED API
def get_fred_data(series_id, api_key):
    url = f'https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={api_key}&file_type=json'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()['observations']
        dates = [obs['date'] for obs in data]
        values = [obs['value'] for obs in data]
        return pd.DataFrame({'Date': dates, series_id: values})
    else:
        print(f"Error fetching data for {series_id}")
        return None

# Create an empty DataFrame to store all treasury data
treasury_df = pd.DataFrame()

# Fetch and merge data for each series
for label, series_id in treasury_series.items():
    data = get_fred_data(series_id, API_KEY)
    if treasury_df.empty:
        treasury_df = data
    else:
        treasury_df = pd.merge(treasury_df, data, on='Date')

# Set 'Date' as the index and convert data types
treasury_df['Date'] = pd.to_datetime(treasury_df['Date'])
treasury_df.set_index('Date', inplace=True)

# Display the data
dataset = treasury_df

scaler = StandardScaler().fit(dataset)
rescaledDataset = pd.DataFrame(scaler.fit_transform(dataset),columns = dataset.columns, index = dataset.index)
# summarize transformed data
dataset.dropna(how='any', inplace=True)
rescaledDataset.dropna(how='any', inplace=True)
rescaledDataset.head(2)

pca = PCA()
PrincipalComponent=pca.fit(rescaledDataset)

NumEigenvalues=5
fig, axes = plt.subplots(ncols=2, figsize=(14,4))
pd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).sort_values().plot.barh(title='Explained Variance Ratio by Top Factors',ax=axes[0]);
pd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).cumsum().plot(ylim=(0,1),ax=axes[1], title='Cumulative Explained Variance');

# explained_variance
pd.Series(np.cumsum(pca.explained_variance_ratio_)).to_frame('Explained Variance_Top 5').head(NumEigenvalues).style.format('{:,.2%}'.format)
```
<br>
<img width="1191" alt="스크린샷 2024-10-01 오후 4 08 45" src="https://github.com/user-attachments/assets/51a28754-6d85-4af1-8906-5505b6191a61">
<br>
-> 데이터 셋을 준비, 스케일 단위 맞추고 PCA분석을 진행한 결과이다. 각 주성분에 따른 분산 설명력이 나타나고 있다


```python
def PCWeights():
    '''
    Principal Components (PC) weights for each 28 PCs
    '''
    weights = pd.DataFrame()

    for i in range(len(pca.components_)):
        weights["weights_{}".format(i)] = pca.components_[i] / sum(pca.components_[i])

    weights = weights.values.T
    return weights

weights=PCWeights()

weights = PCWeights()
NumComponents=3

topPortfolios = pd.DataFrame(weights[:NumComponents], columns=dataset.columns)
topPortfolios.index = [f'Principal Component {i}' for i in range(1, NumComponents+1)]

axes = topPortfolios.T.plot.bar(subplots=True, legend=False,figsize=(14,10))
plt.subplots_adjust(hspace=0.35)
axes[0].set_ylim(0, .2);


plt.plot(pca.components_[0:3].T)
plt.xlabel("Principal Component")
plt.show()
```
<br>

![Unknown](https://github.com/user-attachments/assets/b056d17b-ef3e-4053-86ed-77172d1fd3ad)
<br>

![Unknown-2](https://github.com/user-attachments/assets/33c1c1f6-592b-4c0f-91bf-7a557e6c5172)
<br>

3번쨰 주성분까지만 해도 분산 설명력이 충분하기 때문에 해당 주성분들의 가중치를 시각화한 것이며
<br>
* 주성분 1 : 고유 벡터는 각 특성에 대한 모두 양의 가중치를 가지며 이는 채권 만기에 따라 동일한 방향으로 가중치가 부여됐음을 확인할 수 있다. 즉, 모든 만기가 동일한 방향으로 이동하도록 하는 움직임을 반영하기 때문에 그 방향은 수익률 곡선의 방향이동(평행이동)에 해당한다.
* 주성분 2 : 고유 벡터는 성분의 절반이 음수이고 나머지는 양수. 결과적으로 수익률 곡선의 기울기 이동을 나타낸다.
* 주성분 3 : 양 -> 음 -> 양. 수익률 곡선의 곡률이동을 나타낸다.
<br>
<br>
<br>


```python

pca.transform(rescaledDataset)[:,:2]


nComp=3
reconst= pd.DataFrame(np.dot(pca.transform(rescaledDataset)[:,:nComp], pca.components_[:nComp,:]),columns=dataset.columns)
plt.figure(figsize=(10,8))
plt.plot(reconst)
plt.ylabel("Treasury Rate")
plt.title("Reconstructed Dataset")
plt.show()
```

<br>

![Unknown-3](https://github.com/user-attachments/assets/81d41825-599b-4851-9b5c-7849d3816bfc)
<br>
-> 또한 이렇게 선택한 주성분들을 토대로 역으로 원본데이터와 근사하는 데이터 재현이 가능하다
<br>

# 비지도학습 : 군집화
* 군집화는 데이터의 볼륨을 줄이고 패턴을 찾는 방법이다. 하지만 주성분이라는 새로운 변수를 만드는게 아니라 원본 데이터를 분류한다
* 군집화 알고리즘은 유사한 데이터 포인트로 구성된 부분에 관측치를 할당한다
* 주어진 군집의 항목이 서로 다른 군집의 항목보다 서로 더 유사하도록, 데이터에서 자연스러운 그룹화를 찾는 것이다.

## 예제: 포트폴리오 관리(투자자 군집화)

```python
# Load libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime

#Import Model Packages
from sklearn.cluster import KMeans, AgglomerativeClustering,AffinityPropagation
from sklearn import metrics

#Other Helper Packages and functions
import matplotlib.ticker as ticker
from itertools import cycle

from google.colab import drive
drive.mount('/content/drive)

dataset = pd.read_excel("/content/drive/MyDrive/KHUDA 금융/ProcessedData.xlsx")


# K-means - SSE 평가
distorsions = []
max_loop=40
for k in range(2, max_loop):
    k_means = KMeans(n_clusters=k)
    k_means.fit(X)
    distorsions.append(k_means.inertia_)
fig = plt.figure(figsize=(10, 5))
plt.plot(range(2, max_loop), distorsions)
plt.xticks([i for i in range(2, max_loop)], rotation=75)
plt.xlabel("Number of clusters")
plt.ylabel("Sum of Square Error")
plt.grid(True)
```
<br>

<img width="648" alt="스크린샷 2024-10-01 오후 4 20 03" src="https://github.com/user-attachments/assets/4a14fec9-0692-4c16-8356-1bfaf03e84b5">

<br>
-> 지정한 k에 따른 오류(SSE)를 측정한 것이다. 7개의 군집 이후 꼬임이 발생하는 것을 확인할 수 있다


```python
nclust=7
#Fit with k-means
k_means = KMeans(n_clusters=nclust)
k_means.fit(X)

cluster_output= pd.concat([pd.DataFrame(X), pd.DataFrame(k_means.labels_, columns = ['cluster'])],axis = 1)
output=cluster_output.groupby('cluster').mean()
output

# 인구 통계 특성
output[['AGE','EDUC','MARRIED','KIDS','LIFECL','OCCAT']].plot.bar(rot=0, figsize=(18,5));

# 재무 및 행동 속성
output[['HHOUSES','NWCAT','INCCL','WSAVED','SPENDMOR','RISK']].plot.bar(rot=0, figsize=(18,5));
```

![Unknown-4](https://github.com/user-attachments/assets/413f41ca-4d50-41be-a564-0dcde3a367af)
![Unknown-5](https://github.com/user-attachments/assets/36b29bcd-cdc6-45cb-82ab-b527cfb9da62)

-> 각 군집들에 속해있는 데이터들의 평균 특성 값이 얼마인지 나타나고 있다. 이를 활용하면 군집 특성에 맞는 적절한 포트폴리오를 적용할 수 있을 것이다.



# 강화학습
* 강화학습은 보상을 극대화하고 패널티를 최소화하는 최적의 정책을 통해 최선의 조치를 찾도록 머신을 훈련시키는 접근방식
* 보상을 극대화하는 강화학습의 주요 아이디어는 알고리즘 거래, 포트폴리오 관리 등 재무의 여러 영역에 부합한다
* 강화학습은 불확실하고 역동적인 환경에서 수익률을 극대화하는 에이전트의 개념이 금융 시장과 상호 작용 하는 투자자나 거래 전략과 공통점이 많이 때문에 특히 알고리즘 거래에 적합하다
* 포트폴리오 관리 및 자산 배분에서 강화학습은 포트폴리오 배분 가중치를 동적으로 변경하는 정책을 직접 학습한다


## 강화학습 이론 및 개념
* 에이전트 : 동작을 수행하는 본체
* 동작 : 에이전트가 환경에서 수행할 수 있는 동작
* 환경 : 에이전트가 속해 있는 세계
* 상태 : 현재 상황을 의미
* 보상 : 에이전트가 마지막으로 수행한 동작을 평가하기 위해 환경에서 보낸 즉각적인 반환, 보상 신호는 모델에 즉각적으로 전달되지 않고 에이전트가 수행하는 일련의 동작의 결과로 보상 신호를 반환한다








